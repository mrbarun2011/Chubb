{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is designed to assess some of your data science skills. Explain your answers as clearly as you can. _Every_ step is important (programing, data handling, machine learning, critical thinking and written explanations). \n",
    "\n",
    "You will work on the following project:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will analyze simulated claims and property data and build a model that predicts claim frequency from property attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create a git repo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should submit your work in the form of a git repository that contains:\n",
    "1. your source code\n",
    "2. a `.yml` file specifying your conda environment\n",
    "3. One or more Jupyter notebooks\n",
    "4. a README file\n",
    "\n",
    "Requirements:\n",
    "- **NO** data should be committed to the repo. Only text files and Jupyter notebooks.\n",
    "- The README file must contain a summary of your methodology and results. \n",
    "- Your code and notebook(s) must be commented so that someone else can understand what you did\n",
    "- All code should be written in Python\n",
    "\n",
    "Advice:\n",
    "- If you know how to set annotated git tags, showcase it with one or more tags\n",
    "- Divide your work into different notebooks that represent your main activities. For example, data exploration can happen in one notebook, data preprocessing in a second one, and model training and evaluation in a third one. This is just a suggestion. What matters is that your work can be quickly grasped by another data scientist.\n",
    "- Commit frequently. In particular, avoid changing two different notebooks in a single commit\n",
    "- The person who will review your work will: 1. copy your repo on their computer, 2. reproduce your conda environment, 3. run the notebook in which you train your final model, 4. evaluate your model on held-out data. So make sure all of this works on your end. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create a conda environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- All the notebooks in your repo must use the same conda environment.\n",
    "- Your conda environment must be specified with platform-agnostic `.yml` file. ([This resource](https://conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html#exporting-an-environment-file-across-platforms) can help)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Inspect the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are free to inspect the data in the way you wish. Plots and comments are welcome. However, to avoid over fitting, we recommend you perform your exploratory data analysis on a random subset of the data only. The performance of your model will be measured on our end with held out data that you don't have access to, so overfitting is a risk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data for this project is contained in three distinct files: `policies.xlsx`, `claims.parquet`, `properties.csv`.\n",
    "\n",
    "Each row in the `policies.xlsx` file corresponds to a single policy term (or contract). A policy term is uniquely identified by its policy number `pol` **and** its start date `start` (a term is a time period that falls between a start date and an end date). \n",
    "\n",
    "Each policy covers a set of properties (or buildings). Each property is listed as a row of the `properties.csv` dataset (`prop_id` is a primary key). To know to which policy a property belongs, use the `pol` column. The three property attributes from this project are `age` (the age of the building in year), `state` (a US state), `sqft` (the square footage of the building).\n",
    "\n",
    "Finally, during each policy term, and for each property, claims can occur. Each claim is recorded in the `claims.parquet` dataset. Use `pol` and `start_date` to match back a claim to its policy term, and the `property` column is a foreign key into `prop_id` from the `properties.csv` dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deliverable:** Your README file should contain detailed explanations of how to do the following:\n",
    "1. retrain your final model from scratch \n",
    "2. run your model on held out data (our held out data will be in exactly the same format as the one you are given here) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is to build a claim frequency model that takes as input the attributes of a single property (state, square footage and building age) and outputs the expected number of claims for that building _per unit of exposure_. We would like you to use 1,000 square feet as your unit of exposure. Therefore, if a building is 2,000 square feet, then it corresponds to 2 units of exposure. If your model outputs 3.23 for this building, it will mean that you predict that, on average, this building will generate 6.46 claims per year. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tips**: You are free to pick your modeling approach. We suggest you take inspiration from [here](https://en.wikipedia.org/wiki/Poisson_regression#%22Exposure%22_and_offset) and [here](https://scikit-learn.org/stable/auto_examples/linear_model/plot_poisson_regression_non_normal_loss.html#sphx-glr-auto-examples-linear-model-plot-poisson-regression-non-normal-loss-py). We also recommend you use the [pandas](https://pandas.pydata.org/docs/index.html) and [scikit-learn](https://scikit-learn.org/) libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On held out data, to assess the performance of your model, we will plot a \"lift chart\", which is a graph that contains your predicted claim frequency on the x-axis and the ground-truth (empirical) claim frequency on the y-axis. More precisely:\n",
    "1. on the x-axis, your model output (over the test set) will be binned into deciles (so there will be 10 points).\n",
    "2. on the y-axis, the true average claim count (per unit of exposure) is displayed, where the average is taken over the bins from the previous bullet point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The more the curve aligns with the identity line, the better your model performs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any additional analysis and conclusion or questions that you wish to expose in your Notebook or README file are welcome. In particular, you could mention (with quantitative arguments):\n",
    "1. the different impact that each feature may have on the claim frequency\n",
    "2. the caveats with which one should trust your model's output (if it were to be used for Business purposes for instance) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned earlier, we would like you to write Python code that will read the raw data and transform it in an appropriate way to train and test your model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
